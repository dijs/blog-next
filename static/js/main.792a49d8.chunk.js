(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{111:function(e,t,n){e.exports=n(340)},337:function(e,t,n){},340:function(e,t,n){"use strict";n.r(t);var a=n(0),r=n.n(a),o=n(97),i=n(345),s=n(346),l=n(344),h=n(98),d=n.n(h),c=n(99),u=n.n(c),m=n(100),p=n.n(m),g=["---\npublished: true\ntitle: Github Pages Static Routing\nblurb: Fixing statically hosted Github Pages site routing with directory indexes\nlayout: post\ndate: 02/06/2019\n---\n\nI ran into this issue when using [create-react-app](https://facebook.github.io/create-react-app/) and [react-snapshot](https://github.com/geelen/react-snapshot) together, but it's possible to use this solution for any setup which creates this problem.\n\n### The Problem\n\nAfter building static site assets, you may have rendered HTML files which link to your difference pages. When using Github to host a static site, it is important to structure these files in their own directory with an index. Using this organization, Github will respond with the correct page.\n\n**Stucture before**\n\n    /site\n        /site/index.html\n        /site/pages/a.html\n        /site/pages/b.html\n        /site/pages/c.html\n\n**After structure fix**\n\n    /site\n        /site/index.html\n        /site/pages/a/index.html\n        /site/pages/b/index.html\n        /site/pages/c/index.html\n\nOnly with this structure can you use loose routes such as:\n\n- https://handle.github.io/site/pages/a\n- https://handle.github.io/site/pages/b/\n- https://handle.github.io/site\n\nIn order to maintain this structure, I wrote a small script to fix the original build output from react-snapshot output.\n\n```js\nconst fs = require('fs');\nconst path = require('path');\n\nconst dir = 'build/post';\nconst posts = fs.readdirSync(dir).filter(name => name.endsWith('html'));\n\nfor (const post of posts) {\n  const base = path.basename(post, '.html');\n  try {\n    fs.mkdirSync(`${dir}/${base}`);\n  } catch (e) {\n    console.log(e.message);\n  }\n  fs.renameSync(`${dir}/${post}`, `${dir}/${base}/index.html`);\n}\n```\n\nI hope someone else finds this useful when building static sites hosted on Github!\n",'---\npublished: true\ntitle: New Blog 2019\nblurb: A few notes on the new look and the technology backing it\nlayout: post\ndate: 2/3/2019\n---\n\nWelcome to my new blog.\n\nIt has been quite a long time since updating the look and technology which powers the blog.\n\nIn the past I had used the [Metalsmith](https://metalsmith.io/) site generator. This was a great tool. Really easy to setup and add plugins to, etc.\n\nFor the past couple years I have been developing with [create-react-app](https://facebook.github.io/create-react-app/) as my starter. And wow... it has grown and matured. I can remember the v1 version of CRA. Almost as soon as I started my project I wanted to eject and do my own thing. But now, I do as much as I can to prevent myself the need to eject.\n\nWith this blog, I feel I have pushed some limits! The blog is entirely created with the modern stack CRA gives us. But, I wanted mulitple static pages for SEO reasons. Not just a single page PWA. So using some clever tricks I was able to parse and transform my old blog posts written in Markdown to statically generated SEO friendly routes.\n\nYou can view my project setup in [Github](https://github.com/dijs/blog-next).\n\nOutside of my blog itself, I have been staying busy.\n\nI moved my family to Transylvania.\n<img src="https://vanderdysinromania.files.wordpress.com/2019/01/img_4564.jpg?w=728&h=361&zoom=2" width="100%">\nWe are doing some good here and having quite the adventure. Learn more about that [here](https://vanderdysinromania.wordpress.com/).\n\nWe have been teaching English, programming, music, art, and loads more.\n\nMy goal is to introduce the kids to what is possible with computers, so they can use them to find joy in learning something new.\n\nI apologize for the long time in between posts. I will try to write more in the coming months.\n','---\npublished: true\ntitle: Loomis Head Study\nblurb: Learning how to draw better by teaching a computer the structure of a head\nlayout: post\ndate: 1/31/2019\n---\n\nI recently watched this video and was amazed at how easy it became to sketch out a head shape with all the feature points.\n\n<div class="video-responsive">\n\t<iframe width="560" height="315" src="https://www.youtube.com/embed/wAOldLWIDSM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n</div>\n\nAfter practing a bit, I got the idea of "automating" the rules of the Loomis Head System.\n\nHere is what I came up with. It is a small subset of what you can prototype with the system, but for a few poses it works quite well.\n\n<iframe width="100%" height="800" src="https://jsfiddle.net/fvkq3m4p/embedded/result,js" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\n\n',"---\npublished: true\ntitle: Ares Game\nlayout: post\nblurb: Learning C++ by writing a simple MMO game\ntags: [game, mmo, rpg, development]\ndate: 3/1/2018\n---\n\nI have been wanting to make a proper open world mmo rpg for a very long time. I have started two or three times in the past, but after a few weeks of work and failures, the projects fizzled.\n\nThis time, I am trying to be more realistic. Instead of overwhelming myself with complex features early on, I am starting with the basics, and will build upon those.\n\nTo challenge myself even more, I will be using C++...\n\nI have only ever created one game in C before and many parts of C++ are a mystery to me, but through this project, I have already learned quite a lot, and I hope to learn much more.\n\nBecause this game will be very open, I am working on the different features of the game first, no storyline, no fancy graphics, etc.\n\nI want to start with the difficult parts first, then add in the details.\n\n### Parts of the game I have created so far:\n\n- Isometric 2D World Rendering (multiple layers)\n- Simple creature/character rendering with movement animation\n- Pathfinding / Viewport control\n- Simple chat system\n- NPC's to communicate with\n- Naive multiplayer. Can currently exist in the same world, see each other, move around, and attack\n- Mob nests and mob creatures which drop items\n- Player inventory\n- Simple combat system (works over network)\n- Tilemap/World editor for creating and updating maps\n- Skill trees\n- Quests (only hunting quests currently)\n- Keyboard shortcuts to use menus and skills (will customize later)\n- Resource scavenging\n- Item crafting using resources\n\nIt seems like a lot... for me. But there is still so much to do.\n\nFor my ALPHA version, I still want to have simple PvP, a vendor system, and player respawn working.\n\nI think with all these things working together, I can testing with other people and get a sense of what feels right and what I need to tweak.\n\nLike I said, no story or theme yet, but I wanted to share the main features that distinguish. Yes, I know none of these ideas are unique, but I hope used together properly, they will make an entertaining mmorpg.\n\n### Features:\n\n- Open world (walk ANYWHERE, I want to be able to get lost in the game \ud83d\udc4d)\n- Attack almost anything (not sure on this one yet. I want it to be realistic here, but could cause real problems for eary players. Maybe make this a option a player can TURN ON PvP, or a designated AREA)\n- All items should be crafted / bought / sold by players (should force a real economy, must work together)\n- Player should be able to build structures (not sure on limits yet)\n- NO LEVELS. Improving in the game will be based on learning skills. You can learn skills from any profession in the game. The skill trees will be hierarchical\n\nThat's it for now.\n\nWe keep sharing thoughts and updates. And maybe a video demo soon!\n","---\npublished: true\ntitle: Random Sentences\nblurb: Generating study material for language learning\nlayout: post\ntags: [random, markov]\ndate: 11/22/2014\n---\n\nRecently, I have been trying to learn the Hungarian language. After memorizing a bunch of words, just repeating them to study got boring. I wanted to write a program which randomly created sentences I could use to practice with. Of course, the best practice would be to go find a native speaker near me, but I wanted a challenge...\n\n### Hypothesis\n\nIn order to generate random sentences, I needed some kind of sentence building logic. I have used Markov Chains to build random names in the past, so why would it not work for sentences?\n\n### Data\n\nFirst! I needed data. Thank God for <http://www.gutenberg.org/>. This saved me loads of time finding a bunch of random text.\n\n### Training\n\nThis was a lot of data, so I did not want to load it all in memory at once. Let's use Stream's!\n\nI streamed the huge text data file and counted all the instances of each [bigram](http://en.wikipedia.org/wiki/Bigram) and [trigram](http://en.wikipedia.org/wiki/Trigram). After counting, I sorted each word's adjacent neighbor by the number of instances. This gave me a list of words and their most used adajcent bigram and trigram neighbor.\n\n### Generation\n\nWith this Markov \"link\" data, I was able to generate rudimentary sentences. Nothing made sense, but it was easy to read.\n\nThese sentences are not logical, they are built by choosing what word should statistically come next.\n\n### Ideas for the future:\n\n- Extract/Analyze subject, verb, object in each sentence\n- Use neural networks for training _next_ word\n\n### Code\n\n```js\n//Create Markov links\nfunction createLinks(input, output, n) {\n\tvar links = {};\n\n\tif (!n) {\n\t\tn = 2; // bigram by default\n\t}\n\n\tfunction trainSentence(sentence) {\n\t\tNGrams.ngrams(sentence, n).forEach(function(gram) {\n\t\t\tvar word = gram[0];\n\t\t\tvar neighbor = gram[n - 1];\n\t\t\tif (!links[word]) {\n\t\t\t\tlinks[word] = {};\n\t\t\t}\n\t\t\tif (!links[word][neighbor]) {\n\t\t\t\tlinks[word][neighbor] = 0;\n\t\t\t}\n\t\t\tlinks[word][neighbor]++;\n\t\t});\n\t}\n\n\tfunction read() {\n\t\tvar buffer;\n\t\twhile ((buffer = stream.read())) {\n\t\t\tbuffer\n\t\t\t\t.toLowerCase()\n\t\t\t\t.split(/\\./g)\n\t\t\t\t.forEach(trainSentence);\n\t\t}\n\t}\n\n\tvar stream = fs.createReadStream(input, {\n\t\tencoding: 'utf8'\n\t});\n\n\tstream.on('readable', read);\n\n\tstream.once('end', function() {\n\t\tvar sortedLinks = {};\n\t\tvar keys = Object.keys(links);\n\n\t\tkeys.forEach(function(key) {\n\t\t\tvar words = Object.keys(links[key]);\n\t\t\tsortedLinks[key] = _.sortBy(words, function(word) {\n\t\t\t\treturn links[key][word];\n\t\t\t});\n\t\t});\n\n\t\tfs.writeFile(output, JSON.stringify(sortedLinks, null, '\\t'));\n\t});\n}\n```\n\n```javascript\n//Generate Sentence\nfunction generateSentenceMark2(\n\tbigramInput,\n\ttrigramInput,\n\tlength,\n\tk,\n\tdictionary,\n\tstarter\n) {\n\tvar bigrams = require(bigramInput);\n\tvar trigrams = require(trigramInput);\n\n\tvar current = starter || _.sample(dictionary || Object.keys(bigrams));\n\tvar last;\n\n\tvar sentence = [current];\n\n\tfunction dictionaryContains(word) {\n\t\treturn _.contains(dictionary, word);\n\t}\n\n\tfunction getNeighbor(links, word) {\n\t\tvar neighbors = links[word];\n\t\tif (dictionary) {\n\t\t\tneighbors = neighbors.filter(dictionaryContains);\n\t\t}\n\t\t// neighbors might be null or empty...\n\t\tvar neighbor = neighbors[Math.ceil(neighbors.length * Math.random() * k)];\n\t\t// Go through all of the words if word doesnt have neighbors\n\t\tvar i = 0;\n\t\twhile (!_.has(links, neighbor) && i < neighbors.length) {\n\t\t\tneighbor = neighbors[i++];\n\t\t}\n\t\treturn neighbor;\n\t}\n\n\t// Generate Neighbors\n\t_(length - 1).times(function(i) {\n\t\tvar neighbor =\n\t\t\ti % 2 === 0 ? getNeighbor(bigrams, current) : getNeighbor(trigrams, last);\n\t\tlast = current;\n\t\tcurrent = neighbor;\n\t\tsentence.push(current);\n\t});\n\n\treturn sentence.join(' ');\n}\n```\n",'---\r\npublished: true\r\ntitle: Trees\r\nblurb: Building 2D and 2D trees with SVG and THREE.js\r\nlayout: post\r\ndate: 11/03/2015\r\n---\r\n\r\nI am a sucker for world generation techniques. I decided to mess around with some simple 2d tree generation. By using a simple tree structure and a bit of geometry and calculated the size, position, and direction of the limbs. The leaves are just connected to the end of limbs.\r\n\r\n<iframe width="100%" height="300" src="https://jsfiddle.net/dijs/cxv3L5yw/embedded/result,js" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\r\n\r\nAfter completing the 2d version. I decided with a bit of more geometry, I could create a similar 3d version. Of course, I had to brush up on some 3d math, but the end result was worth it.\r\n\r\n<iframe width="100%" height="300" src="https://jsfiddle.net/dijs/4a0oah5r/embedded/result,js" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\r\n\r\nRefresh to see different trees!\r\n\r\nNow that the 3d math is figured out, I can try to make it look more realistic in the future.\r\n','---\npublished: false\ntitle: CSS Animation\nlayout: post\ndate: 1/25/2019\n---\n\nRecently, I worked on a project which included ....\n\nIn the past, I had mostly handled web animations with either pure javascript or a combination of CSS and javascript. Today, things are radically different.\n\nI will not go deep into describing CSS transforms and animation here, since there are much better places for that:\n\nhttps://cssreference.io/animations/\n<br>\nhttps://cssreference.io/property/transform/\n<br>\nhttps://cssreference.io/transitions/\n\nIf you do not understand these properties yet, go study them first.\n\nIn this article I will discuss some of the pain points I experienced when adding animations to my project, and give some tips and solutions so you will not make the same mistakes.\n\n## Use CSS pre-processors!\n\nThey are really easy to setup. And even easier if you are starting with a boilerplate project like [create-react-app](https://facebook.github.io/create-react-app/) or something similar. They may already have one available. Please check!\n\nBut why?\n\nThere are loads of benefits when using a preprocessor. But more specifically, I noticed a few which helped me when creating and organizing my animations:\n\n- Write your animation styles in separate files. I personally use a separate animation style sheet for each module.\n- Nesting. This helps alleviate CSS selector repetition and also allows for more organized styles IMO.\n- Use variables. It is always good to have standards. You don\'t want a bunch of hard coded values everywhere. This will reduce random buggy values floating around.\n- Loops. When creating styles which use repetitive styles, a pre-processor can automate this for you. A common use case here is staggered animations. Each iteration of the loop may increase the animation delay\n\n## For animations with many stages, use classes to switch animations\n\nFor example:\n\n    none, idle, active, completed\n\nare possible states for a element to have set\n\nYou do not want the `idle` state animation to be the default for your component, because that will cause janky animations on page load. I noticed this when some of my components were fading in every time I refreshed the page. It looked horrible.\n\nA good rule of thumb is to only change animation state when reacting to a user interaction.\n\n## Make your animation duration as small as possible\n\nI normally stick to a **maximum** of 300ms. No one wants to wait for the UI.\n\nYou want to user to think, what just happened? I don\'t know, but it looked great.\n\n<iframe width="100%" height="300" src="//jsfiddle.net/dijs/wym6dfh0/29/embedded/result,css/" allowfullscreen="allowfullscreen" allowpaymentrequest frameborder="0"></iframe>\n',"---\r\npublished: true\r\ntitle: Log with Comments\r\nblurb: Remember complex code by writing good comments\r\nlayout: post\r\ndate: 12/25/2015\r\n---\r\nRecently, I was debugging though a codebase and had inserted a bunch of logging statements in between the problem areas. Normally, after debugging and solving the issues, I would remove the logs and be done with it. Code is working again, done, move on with life...\r\n\r\nWrong!\r\n\r\nIt will bite you. Most of us cannot remember those fine details in our codebase, especially when the logic is any kind of complicated.\r\n\r\nI have made the mistake many times, and it finally clicked for me. If I am already logging information which helps me understand what the program is doing, why not use that for comments? So, instead of ripping out the logs I added after debugging, I just convert them to practical future knowledge comments.\r\n\r\nBy the way, this does not replace following good code standards and using descriptive variable and function/method names, etc.\r\n\r\nExample:\r\n\r\n```js\r\nif(balance - amount > 0) {\r\n  withdraw(amount);\r\n}else{\r\n  throw new Error('not enough money');\r\n}\r\n```\r\n\r\nThere is a bug in this code. You cannot empty your account if you wanted to, because you must leave a balance larger than zero.\r\n\r\nBut by just glancing at it, that may not be immediately understood.\r\n\r\nIf I were debugging this code, I may do this:\r\n\r\n```js\r\nif(balance - amount > 0) {\r\n  console.log('withdrew ' + amount + ' and left ' + (balance - amount));\r\n  withdraw(amount);\r\n}else{\r\n  throw new Error('not enough money');\r\n}\r\n```\r\n\r\nAfter finding the error, I would switch that log statement to a useful comment.\r\n\r\n```javascript\r\n// Withdrawal amount must not exceed account balance, but can equal balance for emptying an account\r\nif(balance - amount >= 0) {\r\n  withdraw(amount);\r\n}else{\r\n  throw new Error('not enough money');\r\n}\r\n```\r\nGo an write some better comments, so you will understand your old code in the future!\r\n","---\r\npublished: true\r\ntitle: Reading List\r\nblurb: Creating ebooks of yearly research history\r\nlayout: post\r\ndate: 12/17/2014\r\n---\r\n\r\nI use [Readability](http://readability.com) to bookmark all the articles I come across every day. Since I do not usually have time to sit down and read during the day, I read at night. Although, I prefer reading physical books to my laptop or phone screen. So, I decided to create books of all my articles per year. They would serve as reading list history and also ease the actual \"reading\" process.\r\n\r\n**Plan**\r\n\r\n- Export all my bookmarks from Readability\r\n- Compile the content of all the articles into a book format\r\n- Create PDF for backup and print\r\n\r\n**Export**\r\n\r\nThis was easy. Readbility just emails you a JSON document with all of your bookmark data. Or you can use their API to gather this information.\r\n\r\n**Compile**\r\n\r\nThis was a bit more difficult. Since the export data did not include the article content, I had to \"parse\" the URL's for content. Luckily, Readability's API includes their parser also.\r\n\r\nI wrote this simple node application to combine the article contents together into a HTML page:\r\n\r\n```javascript\r\n//Create Reading List eBook\r\nvar readability = require('readability-api');\r\nvar async = require('async');\r\n\r\nvar articleUrls = require('./readability.json').bookmarks.map(function(\r\n\tarticle\r\n) {\r\n\treturn article.article__url;\r\n});\r\n\r\nreadability.configure({\r\n\tparser_token: '<API KEY>'\r\n});\r\n\r\nvar parser = new readability.parser();\r\n\r\nvar html =\r\n\t'<!DOCTYPE html><html><head><meta charset=\"UTF-8\"><link href=\"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\" rel=\"stylesheet\"><title>Reading Material 2014</title></head><body class=\"container\">';\r\n\r\nasync.each(\r\n\tarticleUrls,\r\n\tfunction(url, done) {\r\n\t\tparser.parse(url, function(err, article) {\r\n\t\t\tif (article) {\r\n\t\t\t\thtml +=\r\n\t\t\t\t\t'<article><h1>' +\r\n\t\t\t\t\tarticle.title +\r\n\t\t\t\t\t'</h1><p>' +\r\n\t\t\t\t\tarticle.content +\r\n\t\t\t\t\t'</p></article>';\r\n\t\t\t}\r\n\t\t\tdone();\r\n\t\t});\r\n\t},\r\n\tfunction() {\r\n\t\thtml += '</body></html>';\r\n\t\tconsole.log(html);\r\n\t}\r\n);\r\n```\r\n\r\nAfter creating and styling the HTML a bit, I used Google Chrome to print the page to PDF for me. I realize this could have been done within the node app also, but Chrome was quicker at the time.\r\n\r\nNow each year I can have a digital history of what I read and have physical copies to fill my bookshelf with.\r\n","---\npublished: true\ntitle: Vacbot\nblurb: Hacking my Roomba \nlayout: post\ndate: 3/27/2016\n---\n\nMy wonderful wife recently bought me a iCreate 2 hackable roomba. She knows me well...\n\nMany different ideas came to my mind while thinking about what to do with it. Most of them were a bit too grand scale, so I decided to get a feel for the roomba's hackability with a simple project first. I will create a drivable roomba with sight.\n\nIn order to start creating the robot, I did a lot of research and decided to use a raspberry pi for the \"brain\" of the bot instead of an arduino (which I had used before) because I needed WiFi, more CPU, easy power and serial inputs.\n\nThe pi setup was pretty straightforward. Installing the OS was simple with NOOBS. The USB WiFi adapter was easy to plugin and get DHCP, although getting a static IP took a lot of digging through forums to figure out and I am still not sure I completely understand what I did. But it works! Getting SSH setup was easy and made my life much easier.\n\nInstalling the camera to the pi was simple after watching an installation video. And thankfully there are easy to use programs already built for the module.\n\nGetting anything to control the vacuum was a headache. I tried first with my mac, which for whatever reason was not creating a TTY to use for communicating. Thankfully, the pi did create one. Although, when trying to use any of the npm modules built for communication, they didn't work. After digging through the specs of the iCreate 2, it turns out the baud rate is different than the original iCreate. After forking one of the modules and updating the rate, I was good to go.\n\nNext was figuring out video streaming. Lot's of people accomplished this in many different ways, but I think I found a great solution. This [project](https://github.com/jacksonliam/mjpg-streamer) is capable of doing everything I needed. I cloned and compiled it on the pi and it already has a raspberry pi camera module interface built in! With one command, I was streaming high quality video at 30 fps.\n\nNow to find a way to use the power from the iCreate 2's serial cable to power my raspberry pi B+...\n\nI have some ideas, but since I am not an electrical engineer, I am not confident they will work. I plan on trying to use the power wires from the serial cable and using a buck converter to get a constant 5v to a micro usb cable which is plugged into the pi. Then (this is where it gets weird) I want to take the data wires (and ground I think) and splice them to a USB A cable which I can plug into the pi for communicating.\n\nNot sure if this will work, but I will find out when the parts arrive!\n\nThere is a lot more to this than I thought, but things are slowly coming together.\n\n","---\r\npublished: true\r\ntitle: Flashcards\r\nblurb: Reverse enginnered app data to create my own studying system\r\nlayout: post\r\ndate: 12/31/2014\r\n---\r\nThrough my adventures of learning the Hungarian language, flashcards have been a great tool for memorizing new vocabulary. The best open source app I found was [Anki](https://github.com/dae/anki). The program was highly customizable and I found tons of free download-able decks online, full of study material. Downside was the mobile app. It was expensive and did not features I had seen on some other free flashcard apps.\r\n\r\nA friend drew my attention to [Flashcards](https://itunes.apple.com/us/app/flashcards/id478986342?mt=8) by NKO, which was a very fun app to use and it was free!\r\n\r\nI wanted to use this new app for my mobile studying, but there was no way I was going to manually create hundreds of words...on my phone...\r\n\r\nAfter doing a little research into the different applications deck file formats, I felt like hacking. Turned out that both apps just used compressed zip files as their outer shell and renamed the extensions. Anki used a complicated SQLite database to store all of their information for each deck. I quickly was able to export the needed data rows (English and Hungarian columns) to a CSV file. The other app used a FDK file format, which archived a JSON (YAY!) configuration file and data files (Images and Sounds). This format was a bit harder to build.\r\n\r\nMy first run through was simple, I just created a text-only version of my Anki deck. Worked like a champ!\r\n\r\nNext big idea was to add images. Ugh... I had a list of English words, and I needed relevant images to pair them with. First tried with Google... Nope, they cut you off quickly with their API limits. After a few more tries, Flickr turned out to be the best, and the easiest to use. Now I had the images and the word pair data.\r\n\r\nGenerating the JSON configuration file took a while to figure out. Turned out they only allow you to use a max of 100 card decks in the free version of the app. I modified my conversion app to iterate through each word pair and link the images.\r\n\r\nEnd result was awesome. Tons of pretty cards to flip through. Now to practice...\r\n\r\nI was planning on released the source code of this project, was it became VERY domain specific. If anyone has a need for this source, please just [email](mailto:richard.vanderdys@gmail.com) me. I would be happy to give it out.\r\n","---\npublished: true\ntitle: Financial Independence with Mint\nblurb: Combining Mint exported data with my learnings from FI research\nlayout: post\ndate: 4/30/2016\ntags: [financial, independence, money, mint, coding, programming]\n---\n\nLearning how to handle money is an important part of growing up. The sooner we can learn and apply good financial skills, the easier the rest of our financial life will be.\n\nI have been using Mint for over five years now. It is now core part of our budgeting and financial workflow.\n\nRecently came across [madFIentist](http://www.madfientist.com/) and was blown away by the fact that he not only writes great financial blog posts, but also builds tools to go along with them. That is something I can get behind.\n\nI have been looking to use the data Mint accumulated for me and use my coding skills to reduce that data into something more useful.\n\nAfter reading about and using his FI [spreadsheet](http://www.madfientist.com/financial-independence-spreadsheet/), I decided I could use similar formulas to calculate my financial independence data for me without ever having to update the spreadsheet.\n\nMy application reads, parses, transforms and reduces your Mint transactions into Financial Independence information.\n\nCheck it out [here](https://github.com/dijs/mint-fi) and let me know what you think.\n\n*I would love to add more features and create a UI for it, but I just wanted to get something out for now.*\n",'---\npublished: true\ntitle: Self Learning Game\nblurb: Training a neural network to play a simple shoot-em-up game by trial and error\nlayout: post\ndate: 08/29/2017\n---\n\nLong have I been trying to make some time to properly use machine learning to\nplay a game.\n\nIn the past, I tried teaching a neural network to play Mario. That did not go very well. [Link](http://richard.vanderdys.blog/posts/stupid-mario.html)\n\nI believe there were too many inputs. I see what I was thinking, I wanted to give the network our entire visual input. But this is not entirely realistic. When we play games, we are not evaluating each pixel of each frame! We are using our incredible eyes to focus on certain parts of the screen, most notably where the player is and what objects are relatively close to him.\n\nSo, Round 2.\n\nI made a simpler game where the goal is not to get hit by missiles.\n\nTake a look:\n\n<iframe width="100%" height="900" src="https://dijs.github.io/ai-dodger/" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\n\nSo, I wanted to train a network to decide where to move next based on the current player\'s state.\n\nAfter creating the actual game and playing it a bit, I generalized the code to\naccept an AI player.\n\nI went through 3 different machine learning implementations before I actually got something that worked.\n\nFirst, was a hand coded solution which tried to use a table with all possible\ninputs and outcomes, and each cell was updated as to which action was best in that state. This solution did not seem to work at all, although it would be interesting to try again using what I have learned now.\n\nI instead, tried to use a already built solution for neural networks. I had heard great things about ConvNetJS, so I implemented the [Reinforcement Learning Brain](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html) to be my actor. The result was not much better... At this point, I created the "Average Reward Over Time" visualization that you can see on the right of the game. These numbers should be growing as time passes, since the AI is supposed to get smarter. But they were not. So, I scrapped that library.\n\nI went with another [library](http://cs.stanford.edu/people/karpathy/reinforcejs/) that focused only on reinforcement learning. Same author. Since I had already generalized the code, implementing this new library was very straightforward. And almost immediately I saw actual learning. So, after cleaning up the code and putting some lipstick on the game itself, I called it a success.\n\nFinally, an AI player that teaches itself to play a simple game!\n\nFor those who are interested, the training inputs I finalized on were simply an array of binary digits which signified whether there existed enemies relative to the players positions (in a line of sight fashion). This can be easily seen in the game. I have highlighted the inputs as green and red. The small size of the input really improved the learning rate of the AI player. The reward was simply based on whether or not the chosen state kept the player alive or not.\n\nThe HUGE difference here (between this and Mario) if you have not guessed already is that I have the game state, and not just the frame buffer to work with. I imagine I could use a similar solution to teach Mario to survive if I figured out a way to reduce a simplified game state.\n\nAnyways, Thanks for reading if you made it this far!\n',"---\r\npublished: true\r\ntitle: Workflow\r\nblurb: Step by step guide to how you should code\r\nlayout: post\r\ndate: 09/19/2015\r\n---\r\nOver the years, my development workflow has changed tremendously. Whether learning from past problem solving experiences, studying software patterns, or learning from other developers, my view on workflow has changed.\r\n\r\nA brief personal coding history:\r\n\r\n**High School (2007):** No OO, no tests, no code repos, no libraries. I was re-inventing the wheel wherever I went.\r\n\r\n**College:** Object oriented design, basic patterns.\r\n\r\n**First Job:** Libraries. Still developing on production code, haha...\r\n\r\n**Second Job:**  Started open source contributing. Used SVN (gross).\r\n\r\n**Current Job:** Tests. Modular design patterns. Git.\r\n\r\nThere it is, eight years of learning.\r\n\r\nI would like to share my current workflow, one that I use for almost all of my projects currently, and has served me well.\r\n\r\n### 1. State the problem or idea\r\n\r\nThis is the first and most important step. You must clearly understand your problem or idea. This is **not** the time to think about implementation! What you are doing here is defining the input and expected output. Think of use cases your solution will be solving.\r\n\r\n### 2. Design and Draw Solution\r\n\r\nYou should spend a lot of time on this one. Find a whiteboard, and get drawing. Define where the data is coming from, and where it is going. Draw how things are connected and what information will be sent between them. Think about the logic expressions you will need. If your data will need to be transformed, draw some examples. At this stage, you are writing down all your design ideas.\r\n\r\n### 3. Modularize Design\r\n\r\nBefore you start on this one, unless you are lucky enough to have two white boards, take a picture of your original solution design. During this step, you are going to simplify and modularize your solution. Start by extracting functionality into independent modules. This allows them to have a clear purpose. What you are doing in this stage is breaking down larger problems into smaller problems. This has many great side effects including easier testability, reusability, and maintainability.\r\n\r\n### 4. Create unit tests for modules\r\n\r\nLet\u2019s make sure the modules will work as expected after we implement them. Since you have the designs for clear and independent modules now, think about the required parameters of each module and their respective required results. Now write unit tests based on your module requirements.\r\n\r\n### 5. Implement modules\r\n\r\nThis one should be straight forward. Implement modules which are validated by your units tests. You might be thinking I am a full on \u201cTest Driven Development\u201d dude by now. I\u2019m not. I just picked up ideas from my peers at work and molded some of their philosophies into my development workflow. \u201cUse only that which works, and take it from any place you can find it.\u201d - Bruce Lee\r\n\r\n### 6. Create use case tests\r\n\r\nNow that you have your modules created and tested, let\u2019s write use case tests. These will be end-to-end test scenarios which cover your entire solution. They will touch many of yours modules.  That being said. I am an advocate of **really really fast tests**, so make sure you mock, stub, do whatever you need to do in order to **not** call actual outside services. That wastes time.\r\n\r\n### 7. Tie together modules to implement solution\r\n\r\nBy the time you get to this step, you should definitely know how your modules should be connected, which coding patterns you will use, and the overall structure of the codebase. Just to point out, this is **not a waterfall** workflow, be agile! If you you need to step back in the process, do it, but **follow the order** of the stages. Do not implement code before you design and write the initial tests for it. Once your use case tests pass, your solution should be complete.\r\n\r\n","---\npublished: true\ntitle: Functional Programming\nblurb: Start writing better code with these tips\nlayout: post\ndate: 1/31/2016\n---\n\nAs I learn more about programming through the years, I find it incredible how the techniques I have come to love are normally very old.\n\nI love math. Always have.\n\nMath inherently does not have extra state laying around. I believe that when we transform our application's state, nothing external of our transformation context should affect the resultant state.\n\n### What do I mean by this?\n\nHere is a simple example: *Let's create a function which computes the sum of a list of numbers.*\n\nHere is an example which uses extra dangerous state (**DON'T DO THIS!**):\n```js\n// This is the external data which\n// could be affected by other code\n// in this context\nlet sum = 0\nfunction computeSum (numbers) {\n  numbers.forEach(n => sum += n)\n}\ncomputeSum([1,2,3])\nconst result = sum\n```\n\nNow, a better example, using a more functional technique:\n\n```js\nconst bySum = (sum, n) => sum + n\n// This is safe from any\n// external code\nconst sum = numbers => numbers.reduce(bySum)\nconst result = sum([1,2,3])\n```\n\nWith functional programming, we can trust our code more. Functional code is more maintainable, predictable, doesn't mutate other code, can be reused, smaller, and sometimes more readable (depending on who is reading it).\n","---\r\npublished: true\r\ntitle: Mutation Testing in Javascript\r\nblurb: Automatically find bugs in your unit tests by slightly mutating your source code\r\nlayout: post\r\ndate: 03/01/2015\r\n---\r\n\r\nMutation testing is hard... But totally worth it!\r\n\r\nI recently learned about the concept from a colleague. For those who might not know what it is:\r\n\r\nTesting by mutation calculates the durability of your unit tests against mutated versions of your source code. These mutant versions are designed to be the result of common programming errors. Since there are many different types of errors, this means that normally a large number of mutants are created and tested. These mutants would be extremely difficult to create by hand, so mutants are generally automatically generated from changing the source code using common *error* patterns.\r\n\r\nError pattern examples may include switching conditional, logical, and arithmetic operators. Possibly even changing the value of literals within the code. These are just a few ideas, and I am hoping one day we can have a more *standardized* set of patterns to use.\r\n\r\n### How it works\r\n\r\nA (hopefully obvious) perquisite to mutation testing is that your unit tests need to **pass** against your original source code. Also, the faster your unit tests are, the easier mutation testing will be.\r\n\r\nSo, step by step:\r\n\r\n1. A baseline unit test is run against your original source code\r\n2. Mutated versions of your code (**mutants**) are generated by changing the code slightly\r\n3. Each mutant is ran against the same unit tests\r\n4. If a mutant passes the test suite, it is considered **killed**\r\n5. Your **score** is calculated by how many mutants were killed against how many were created\r\n\r\nThe goal is to try to kill all the mutants.\r\n\r\n### By example...\r\n\r\nHere we have a very simple Javascript module:\r\n\r\n```javascript\r\n//Bank Account Module\r\nmodule.exports = function() {\r\n\tvar balance = 0;\r\n\tthis.getBalance = function() {\r\n\t\treturn balance;\r\n\t};\r\n\tthis.deposit = function(amount) {\r\n\t\tbalance += amount;\r\n\t};\r\n\tthis.withdraw = function(amount) {\r\n\t\tif (balance >= amount) {\r\n\t\t\tbalance -= amount;\r\n\t\t\treturn true;\r\n\t\t} else {\r\n\t\t\treturn false;\r\n\t\t}\r\n\t};\r\n\treturn this;\r\n};\r\n```\r\n\r\nEasy mutation could be to change the balance literal to initialize to -1.\r\n\r\nIf our unit tests started each check with a clean Account instance and assumed that the balance should be 0, our tests would fail.\r\n\r\n```javascript\r\n//Unit Test (Wrong)\r\nit('should deposit', function() {\r\n\taccount.deposit(50);\r\n\taccount.getBalance().should.equal(50);\r\n});\r\n```\r\n\r\nThe corrected unit test might look something like this:\r\n\r\n```javascript\r\n//Unit Test (Corrected)\r\nit('should deposit', function() {\r\n\tvar initial = account.getBalance();\r\n\taccount.deposit(50);\r\n\taccount.getBalance().should.equal(initial + 50);\r\n});\r\n```\r\n\r\nThis is **one** example where mutation testing could help.\r\n\r\n### So I created something\r\n\r\nIn order to better understand and learn about mutation testing, I wanted to write my own Javascript mutation tester. I found one project for mutation testing in Javascript, and although I am normally a fan of \"don't re-invent the wheel\". In this case, the tool was only usable for *grunt* built projects. I wanted to create a more general use tool which was (build system) agnostic.\r\n\r\nBig hurdle I hit with writing this was actually \"node-specifc\" issues. While mutating source code, loading and running tests, I kept getting the same results. After hours of beating my head against the wall, I tried renaming the files before loading them in. Sure enough, it worked. So in doing this project I learned a bit more about how the node.js **require** method works and how it caches files.\r\n\r\n[Here](https://github.com/dijs/profx) is the project.\r\n","---\npublished: true\ntitle: Starters\nblurb: Start projects with ease\nlayout: post\ndate: 01/23/2016\n---\n\nEver been under pressure to throw up a react/redux application in 20 minutes?\n\nGood luck!\n\nI love the modern technology stack we have today for building isomorphic javascript applications. Webpack, Babel, React, Redux... can't get enough of them.\n\nBut when you are staring at a blank editor and need to code something as quickly as possible, getting the initial environment up usually can take longer than coding actual application. With the time futzing with accidental typos in config files, creating the directory structure, finding and downloading dependencies, etc. You could have already had a good wireframe with basic functionality coded.\n\nThis is why I created a few application starters.\n\nBabel: https://github.com/dijs/babel-starter\n\nReact: https://github.com/dijs/react-starter\n\nRedux: https://github.com/dijs/redux-starter\n\nI hope these are useful.\n",'---\npublished: true\ntitle: Yet Another Container/Presenter Pattern\nblurb: Use another component layer to transform your data to be rendered easily\nlayout: post\ndate: 11/08/2016\n---\n\n_These ideas are not just for React, but for simplicity, I will be using React to demonstrate._\n\nI am not going to go over what Smart/Dumb components are, Dan Abramov already did a great job [here](https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0#.nrqfzj2dj).\n\nThese are awesome ideas! And I would like to expand upon it.\n\nBefore I start, let me be clear. This pattern I will be explaining is **NOT** for every little component.\n\nWithout future ado...\n\nIntroduce another level of complexity \ud83d\ude04!\n\nYes, it sounds ridiculous, but here is the reason: **Data which is easy to store and update is not always structured in an efficient manner to pass down through components or for the components to use and reduce that data.**\n\nMy proposition is to use a **View Container** in between the Data Store and the Presenter.\n\n![Pattern Flow](https://docs.google.com/drawings/d/1SLtlXyZzw6CukM2CigBJOyMpq9Yny57p9-L7sIonKA0/pub?w=629&h=195)\n\nBy splitting up the way we manage the data and the way we manage the view state we can use better data structures to store, update, and obtain the necessary data to present.\n\nHere are some examples of the same application, but handling data, reducing, and rendering differently.\n\n### First is our monolith. Do everything in one component.\n\n<iframe width="100%" height="300" src="https://jsfiddle.net/fojjyLkk/1/embedded/js,result" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\n\n### Second is using the plain Container/Presenter pattern.\n\n<iframe width="100%" height="300" src="https://jsfiddle.net/cnmLyqx7/1/embedded/js,result" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\n\n### Third example shows Data Store/View Container/Presenter pattern.\n\n<iframe width="100%" height="300" src="https://jsfiddle.net/d6nc2u9t/embedded/js,result" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\n\nThe state and action handler in the last example can be extracted out as a Redux Store/Reducer, which would clean up this example even more.\n\nI hope my explanation showed how we can improve the separation of concerns and logic in our applications.\n\n<hr />\n\n## 2019 Update\n\nA much better way to acheive this same pattern is by using a [react-recollect](https://github.com/davidgilbertson/react-recollect) store and transforming your view data with `selectors`.\n','---\npublished: true\nblurb: Automating personal object detection for home security system\ntitle: Homemade Thing Recognizer\nlayout: post\ndate: 11/19/2017\n---\n\nWhen I was in college (around 2010), I worked with OpenCV and facial detection quite a bit, I always found it very interesting. I wanted to build a recognition platform on top of this, but I quickly determined I was in **way** over my head...\n\nNeural networks have come a long way since then.\n\nSince moving into our home, I have been trying to find ways of gathering data. Temperatures inside and outside, moisture levels in the garden, todo list data, camera systems with motion detection, etc. I have been doing this in the hope that one day, I can create a neural networks to help with things around the house.\n\nRecently, I have had a breakthrough in this area.\n\nAfter noticing that I was _accidentally_ storing hundreds of gigabytes of motion detection videos, I realized the treasure this could be!\n\n## Finding the interesting bits\n\nSince I peruse the tech blogs every day, I remembered seeing some new work in the image tagging world. After some searching, I found it again, [YOLO](https://pjreddie.com/darknet/yolo/). After building this library, downloading a previously trained model, and learning a bit of python, I was labeling objects in my videos!\n\nAt first, I naively thought there was no way this could work for my use case. Each tag took around 30 seconds to calculate and I was potentially going to have thousands of frames thrown at this thing.\n\nNO. I went a **much** simpler route that has bene working well.\n\n## Only a few frames\n\nI decided there was no reason to analyze entire videos worth of frames. My motion detection was already doing the hard work for me. I decided to break out **ffmpeg** and extract a few frames of the detection video.\n\nThis greatly simplified my problem. Now all I had to do was tag a few images for each detection video.\n\nI quickly wrote a simple python server that accepted an POST request with an image, and used YOLO to spit out a list of labels. Each which had bounding box data... this was getting exciting.\n\n## Tagging\n\nJust for fun, I whipped up a quick react app which allowed me to _swipe_ through all my tagged objects and give them **real** labels.\n\n## Personal Recognition\n\nTo be honest, the labels were nice, but those were not what I was after. I wanted to know when my wife walks through the gate, if my dog is barking at the fence, or if a friend comes over. That was my goal.\n\nI went back to a library I had tried before and failed with. [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/docs.html) I had remembered he made a demo which could learn to recognize objects from a dataset with 90% accuracy after just a few minutes.\n\nHere is the [demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html).\n\nI now had a large amount of raw training data, and a potential library to use.\n\nI knew my raw data needed to be cleaned up and normalized for it to run through a network. I kept it simple, I **cropped** out the detected boundary of the image then used a **contain** algorithm to resize them to 64x64. In the future I want to also save a horizontal flipped version of the image so I can double my training data.\n\nIt took a fews day to figure out ConvNetJS. At first I was only running the data through once per image and expecting it to work \ud83e\udd26.\n\nI went back to the demo and realized he was throwing random input images at the network and teaching it the correct label, while minimizing the "classification loss". I coded up something very similar, but without the fancy graphs. It was just a terminal app that took all my normalized images and personal labels and trained them randomly over a set amount of iterations.\n\nAfter tweaking the network training configuration a few times, my training normally takes around 30 mins, and performs quite well.\n\nI now get emails throughout the day of **recognized** people, dogs, cars, etc.\n\nStill more work to do, but I am very happy with where it is right now.\n','---\r\npublished: true\r\ntitle: Panhandling\r\nblurb: How much money could you make begging on the streets?\r\nlayout: post\r\ndate: 12/16/2015\r\n---\r\n\r\nAfter hearing plenty of stories about "fake" beggars, I decided to mathematically figure out how much one could earn by begging an entire year.\r\n\r\n### Formula\r\n![Formula](https://latex.codecogs.com/gif.latex?%5Csum_%7Bd%3D1%7D%5E%7B365%7D%20%5Cleft%20%5B%5Csum_%7Bh%3D1%7D%5E%7B24%7D%20e%5E%7B-%5Cleft%20%28%20%5Cfrac%7Bh-12%7D%7B6%7D%20%5Cright%20%29%5E2%7D%20%5Cright%20%5D%20%5Ccdot%20%5Cfrac%7B%5Csin%20%5Cleft%20%28%5Cfrac%7Bd%7D%7B7%7D%20%5Cright%29%20&plus;%201%7D%7B2%7D)\r\n\r\n### How I got there...\r\n\r\nI am using a bell curve function to find the estimates of earning throughout each day.\r\n\r\nThen I use a wave function to find daily earning estimates throughout the week.\r\n\r\nAfter using these two functions to transform the daily earnings, I calculate the summation of an entire year.\r\n\r\nI realize that I could use *much* better functions for estimation, but this was just some quick research.\r\n\r\nFinally, if you could find a way to earn $20/hr, you could make $39,030.67 annually according to this formula.\r\n',"---\r\npublished: true\r\ntitle: Stupid Mario\r\nblurb: Learning neural network algorithms with Mario and emulators\r\nlayout: post\r\ndate: 12/01/2014\r\n---\r\n\r\nI have seen a bunch of neural network articles around Hacker News lately. The way I boiled it down was: if you could simplify a problem and solution down to a finite number of inputs and a binary output, you could train a network to solve that problem on its own.\r\n\r\nI used to do a lot of Mario speed runs in high school. I was never any good, but to this day that is the only way I can play Mario now. It has been ingrained into my muscle memory.\r\n\r\nWhat if I taught a neural network how to play Mario?\r\n\r\n**Hypothesis**\r\n\r\nStarting simple. I just want to see if I can train a network to NOT DIE. I will ask the network when to jump throughout the level. After so many experiences dying where he should jump, hopefully he will learn to jump.\r\n\r\n**Setup**\r\n\r\nI used the fantastic [JSNES](https://fir.sh/projects/jsnes/) to facilitate the game itself. I was able to easily grab the data I needed directly from the emulator's memory. I found this great [map](http://datacrystal.romhacking.net/wiki/Super_Mario_Bros.:RAM_map) that made it easy to find the exact variables I used for training and event detection.\r\n\r\nThe next part was the brain itself.\r\n\r\nI tried quite a few big open source neural network Javascript libraries, but due to development issues, I decided to just use a simple perceptron function.\r\n\r\n**Training**\r\n\r\nI wanted to training to be just like a human, visual. So my training input was a normalized frame buffer which I converted to greyscale for ease of use with the perceptron.\r\n\r\nThe output would be if Mario needed to jump right after that frame.\r\n\r\nI simply trained the perceptron by detecting when Mario had died and used the previous buffer.\r\n\r\nAutomating the game play was the trickiest part. I had to detect game overs, collision deaths, and pit fall deaths, and automate Mario's controls.\r\n\r\n**Outcome**\r\n\r\nNo, I do not have a perfect Mario player yet... that is why I titled this post _Stupid_ Mario.\r\n\r\nIt is hard to truly tell if a network is learning until it makes sufficient progress.\r\n\r\n[Try it out!](http://www.richardvanderdys.com/projects/stupid-mario)\r\n\r\n**Future Ideas**\r\n\r\n- Use more events for training\r\n- Save/Load network data\r\n- Use a big name network library\r\n- Other games\r\n",'---\r\npublished: true\r\ntitle: Hotspots\r\nblurb: Using git history and some Google research to find problem areas in a codebase\r\nlayout: post\r\ndate: 04/09/2015\r\n---\r\nI recently wrote a little tool that implements a simple algorithm proposed in [this](http://google-engtools.blogspot.com/2011/12/bug-prediction-at-google.html) Google article. The idea is you can analyze your codebase and score each source file with a "bugginess".\r\n\r\nThe problem we are trying to solve here is to mitigate the common "Look\'s good to me" when code reviewing.\r\n\r\nIt is easy to understand why this problem exists. As any codebase and team grows, this problem grows in turn. With many developers changing source code over time, the learning curve for the codebase becomes more difficult. This problem can arise when the code reviewer does not fully understand the language, pattern, organization, structure, and history of the code.\r\n\r\nCode history holds a ton of knowledge. We humans would have a hard time holding the full history of a codebase in our head though... so let\'s automate it.\r\n\r\nIf we can figure out how buggy a source file is, a code reviewer can be confident in knowing how dangerous their "Look\'s good to me" comment will be. For example, if a file\'s bugginess score is very low, the reviewer can quickly look over the changed content and know that this file has rarely caused issues in the past. Although, on the other hand, if the file has a high score, the developer might want to ask for more developers to review the code being changed.\r\n\r\nSo how can we figure out the score of a source file?\r\n\r\nIn the original implentation, the algorithm used purely git commit messages to figure out if the files being changed were related to a bug fix. In my organization, we do not have that luxury. Our commit messages have evolved over time and we have used other tracking tools to communicate the purpose of code changes. Thankfully we have years of history in our tracking tool. And each story tied to code changes has a "type" we can filter by.\r\n\r\nNow we have our commits related to bug fixes.\r\n\r\nIn order to score each file, we could just count the number of commits that changed said file. Although, a Google engineer came up with a fancy formula for value-over-time which we will use instead.\r\n\r\n![t = 0..1](/hotspots-formula.gif)\r\n\r\nThe commit time is normalized by treating 0 as the first commit time and 1 as the current time. For each commit, this score is added to the changed file\'s total score.\r\n\r\nAs for the implentation, I used local git commands instead of hitting Github\'s API for two reasons:\r\n\r\n- Speed of commit searching and information retrieval\r\n- Did not want users to have to hassle with another API token\r\n\r\nAfter building the data, I sort it and output the result to a CSV file for ease of visualization.\r\n\r\nI simply dragged it into Google Drive and created a chart in seconds, which showed where our dangerous code lived.\r\n\r\nCheck out the [code](https://github.com/dijs/hotspots)!\r\n\r\n### Future Ideas\r\n\r\n- Integrate more tracking tools\r\n- Use Github Issues as a source of data\r\n- Allow for custom value-of-time formulas\r\n- Modularize the commit gathering (choose local git, github, bitbucket)\r\n','---\r\npublished: true\r\ntitle: Parsz\r\nblurb: Parse HTML with a JSON based structure request\r\nlayout: post\r\ndate: 01/25/2015\r\n---\r\nI have been always been entranced with how much data is publicly available on the web. Although, most of the time the data is not directly usable for other purposes than reading. With so many HTML parsing libraries out there, it is common practice to build an application for a particular site and a specific data set. Parsz tries to generalize the "parsing" and focus on the data structure you need. Instead of downloading, processing, and transforming the data each time you need to parse a web page, use parsz. Enough of the TV commercial...\r\n\r\nI did NOT originally think of this. [This guy](https://github.com/fizx/parsley/) did. I saw it, thought it was a great idea and decided such a simple idea could be implemented much easier by using Node.\r\n\r\nThe idea that caught me was to not re-implement the same dang code over and over again (possibly with stupid mistakes) in order to preform data parsing on a web page. Looking back, it makes me feel stupid. HTML has for the most part, been standardized. Why did I never think to build a engine which would use data/element or data/attribute relationships to build a recipe for parsing needed data. I really like the notion of labeling the data by name and location and having "someone" else do the work.\r\n\r\nSince JSON has become very popular, I have not built in XML based output yet, although that may be just a npm module away.\r\n\r\nThere are many improvements and features to add, like regular expression support and custom functions. But overall, the tool/library (however the community decides to use it) is in a fairly usable state.\r\n\r\nHere is the [repo](https://github.com/dijs/parsz).\r\n\r\n\r\nHave fun. Make sure to follow T&C.\r\n','---\r\npublished: true\r\ntitle: Text Analysis with React\r\nblurb: Early learning days with React communication\r\nlayout: post\r\ndate: 12/26/2015\r\n---\r\nOkay. I created a sample React app which has a live text analyzer.\r\n\r\nI think building the components within Javascript is cool, and would be up for doing that more.\r\n\r\nCommunication between components is crazy messy though. Maybe I am not doing it correctly, but even the documentation was confusing. In order to bind two components to listen and use a data element, I had to use vanilla Javascript event listeners and dispatchers\u2026 Really?\r\n\r\n### Bottom line\r\n\r\n- Building app structure/markup in Javascript is a great idea.\r\n- Breaking down your app into simple testable components is wonderful\r\n- Component communication is down right confusing\r\n\r\n### Check it out\r\n\r\n<iframe width="100%" height="900" src="https://dijs.github.io/React-Text-Analysis" allowfullscreen="allowfullscreen" frameborder="0"></iframe>\r\n\r\n\r\n','---\npublished: true\ntitle: Hue Beats\nblurb: Fun with music and Phillips Hue\nlayout: post\ndate: 1/24/2016\n---\n\nI wanted to check in and show off my latest Hue project.\n\nSince I am a developer/musician, I really enjoy messing with sound on the computer. Growing up, I adored all the music visualization programs we had. Very cool stuff.\n\nAfter recently using the Hue lights to code a custom alarm system, I had the idea to create my own music visualization program. This is my first time, so...\n\nI learned quickly, decoding audio data is tough. Or maybe I am missing something.\n\nI could not find an audio library for node that just worked. One had a binary dependency that wouldn\'t compile on my OSX. I ended up just using a node version of the WebAudio API, which is really cool. But still, I am an audio processing n00b.\n\nAnyways, I got something working even though the timing is a little off.\n\nEnjoy.\n\n<video width="320" height="240" controls>\n  <source src="/videos/hue-beats-480.webm" type="video/webm">\nYour browser does not support the video tag.\n</video>\n','---\r\npublished: true\r\ntitle: Pivotal Charts\r\nblurb: Automated project managment charts with Pivotal API and D3\r\nlayout: post\r\ndate: 01/18/2015\r\n---\r\n\r\nDuring my day job, visualizing our development progress is very helpful for the team. Personally, I am not good at choosing which way to visualize this data, but once seen, I understand the value of them. We recently acquired a project manager for the team. He spent time creating charts of our sprint progress by hand, almost every day. While these charts were very useful and telling, my engineering brain could not let go of the fact that we should be able to see this, anytime, with any teams data. So I made it.\r\n\r\nI have started with only one graph. But I believe it is a important graph to have. It shows the progress of sprint stories over time. Our developers use it on a daily basis, during our stand ups. The project managers have been the key user of this project and have been very helpful. They can instantly find ways to make it more useful for them and the team.\r\n\r\nEnough backstory, lets talk code.\r\n\r\n### Development\r\n\r\nWe use [Pivotal](https://www.pivotaltracker.com) here to track our stories (bugs, chores, features...) and they have a pretty decent API. I had used their API on many other occasions creating internal apps for our team. But this project seemed to be begging to be open source. I generalized the data gathering and processing as much as I could. The filters in the app itself should work for any team. I stuck to a node app for ease of development, since I had already written a module for the Pivotal API. I got lots of practice with the use of [underscore](http://underscorejs.org/) chaining for data processing. By finding the ways to filter/map/group your data, you can truly make your code pretty and easy to understand. The difficult part was actually transforming the data to work with the chart [library](http://nvd3.org/). By the way, if anyone can see a better way to transform the data, please let me know! The charting library I used was a powerful tool, but hard to find documentation for. I believe the project owner has switched/forked a few times.\r\n\r\n<img src="/pivotal.png" width="100%">\r\n\r\n### Future\r\n\r\nI believe this project can be expanded upon in a big way. I am sure many other teams have their own favorite graphs. I would love to see the community help add those graphs. I plan on making a "dashboard" index page to show off all the graphs we currently manage. This is of course limited to teams who use Pivotal, but even that may be generalized out one day...\r\n\r\nHere is the Github project: [https://github.com/dijs/pivotal-charts](https://github.com/dijs/pivotal-charts)\r\n',"---\r\npublished: true\r\ntitle: The Middleman\r\nblurb: Tiny server which proxies and modifies responses of another server for testing purposes\r\nlayout: post\r\ndate: 04/28/2015\r\n---\r\nSo... are you mobile friendly according to Google?\r\n\r\nI was recently working on fixing some mobile issues at my place of work. I was required to push to production for each change since our development servers are locked down and not visible to Google.\r\n\r\nIn order to quickly make changes and test them I needed another development environment visible to Google with a clone of the website I was working on.\r\n\r\nMy first idea was to proxy the website through a small Node proxy server and modify the response. This kinda worked, but I ran in to too many brick walls...\r\n\r\nI needed more control.\r\n\r\nWhy not just mirror the response?\r\n\r\nWhat I ended up with was a application that would cache a HTTP response for a given URL and modify the page by adding CSS and Javascript to the response. If there are any dependencies, the server will proxy them through with a little Express magic.\r\n\r\nCheck out the [source](https://github.com/dijs/middleman).\r\n\r\n### Future\r\n\r\n- Add a edit page to accompany the view, currently you just make another middleman page\r\n- Add more modification options\r\n"].map(function(e){return d()(e)}).filter(function(e){return e.metadata.published}).map(function(e){return e.metadata.date=new Date(e.metadata.date),e}).sort(function(e,t){return+t.metadata.date-+e.metadata.date}).map(function(e,t){return e.metadata.blurb||(e.metadata.blurb="Write something here!"),e.metadata.date=p.a.format(e.metadata.date,"MMM D, YYYY"),e.slug=u()(e.metadata.title),e.index=t,e.path="/post/".concat(e.slug),e}),f=n(101),w=n.n(f),y=n(29),b=n.n(y),v=n(342);function I(){return(I=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e}).apply(this,arguments)}var k=r.a.createElement("title",null,"GitHub icon"),j=r.a.createElement("path",{d:"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"}),E=function(e){return r.a.createElement("svg",I({role:"img",viewBox:"0 0 24 24"},e),k,j)};n.p;function T(){return(T=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e}).apply(this,arguments)}var x=r.a.createElement("title",null,"LinkedIn icon"),S=r.a.createElement("path",{d:"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"}),A=function(e){return r.a.createElement("svg",T({role:"img",viewBox:"0 0 24 24"},e),x,S)};n.p;function N(){return(N=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e}).apply(this,arguments)}var C=r.a.createElement("title",null,"NPM icon"),M=r.a.createElement("path",{d:"M0 7.334v8h6.666v1.332H12v-1.332h12v-8H0zm6.666 6.664H5.334v-4H3.999v4H1.335V8.667h5.331v5.331zm4 0v1.336H8.001V8.667h5.334v5.332h-2.669v-.001zm12.001 0h-1.33v-4h-1.336v4h-1.335v-4h-1.33v4h-2.671V8.667h8.002v5.331z"}),z=r.a.createElement("path",{d:"M10.665 10H12v2.667h-1.335V10z"}),O=function(e){return r.a.createElement("svg",N({role:"img",viewBox:"0 0 24 24"},e),C,M,z)};n.p;function B(){return r.a.createElement(v.a,{to:"/",className:"back"},"Back to all posts")}var P=n(108),L=n(102),H=n.n(L);function R(){return(R=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e}).apply(this,arguments)}var W=r.a.createElement("g",null,r.a.createElement("line",{className:"st2",x1:212.6,y1:390.2,x2:212.6,y2:390.2}),r.a.createElement("line",{className:"st2",x1:161,y1:390.2,x2:161,y2:390.2}),r.a.createElement("line",{x1:303.1,y1:192,x2:303.1,y2:141.9}),r.a.createElement("line",{x1:123.1,y1:390,x2:100.1,y2:377}),r.a.createElement("line",{x1:99.7,y1:376.8,x2:99,y2:376.4}),r.a.createElement("path",{className:"st2",d:"M146.5,390"}),r.a.createElement("path",{className:"st2",d:"M122.8,390"}),r.a.createElement("path",{className:"st3",d:"M258.9,335.6l-5.7-9.8l-12.5-21.6l-7.8-13.6h-91.5l-8.1,14.1l-11.9,20.6l-6.4,11.1l-44.4-46.3v69.8l29.2,16.8 l0.4,0.2l23,13.2l63.7,36.8l63.8-36.8l24.3-14.1l28.2-16.2v-69.8L258.9,335.6z M228.8,390.2h-16.2l14.1-24.5l-20-44.6h-39.9 l-20,44.6l14.1,24.5h-15.1l-31.4-34.5l7.8-12l6.1-9.5l16.3-25.2l1.2-1.9l40.9,2l42-2l1.2,1.9l16.5,25.4l5.4,8.3l8.4,12.9 L228.8,390.2z"}),r.a.createElement("polygon",{className:"st1",points:"146.9,365.7 161,390.2 145.9,390.2 114.5,355.7 122.2,343.8 128.4,334.3 144.7,309.1 145.9,307.3  150.1,307.5 147.5,309.1 166.9,321.2 \t"}),r.a.createElement("polygon",{className:"st4",points:"260.2,355.7 228.8,390.2 212.6,390.2 226.7,365.7 206.8,321.2 227.1,309.1 224.5,307.5 228.8,307.3  230,309.1 246.4,334.5 251.8,342.8 \t"}),r.a.createElement("polygon",{className:"st1",points:"303,141.8 303,290.1 258.9,335.6 253.2,325.7 240.7,304.1 232.9,290.5 141.4,290.5 133.3,304.6  121.4,325.3 115,336.4 70.5,290.1 70.5,141.8 118,92.4 119.3,91 137.8,72.6 235.8,72.6 254.3,91 274.9,111.7 \t"}),r.a.createElement("polygon",{className:"st3",points:"303,73.9 303,141.8 274.9,111.7 254.3,91 235.8,72.6 137.8,72.6 119.3,91 118,92.4 70.5,141.8  70.5,73.9 186.8,5.8 \t"}),r.a.createElement("g",{className:"st5"},r.a.createElement("polygon",{className:"st6",points:"303,73.9 303,141.8 274.9,111.7 254.3,91 235.8,72.6 186.8,72.6 186.8,5.8 \t\t"})),r.a.createElement("polygon",{className:"st4",points:"303,141.8 303,290.1 258.9,335.6 253.2,325.7 240.7,304.1 232.9,290.5 186.8,290.5 186.8,72.6  235.8,72.6 254.3,91 274.9,111.7 \t"}),r.a.createElement("polygon",{className:"st7",points:"186.8,309.3 186.8,321.2 166.9,321.2 147.5,309.1 150.1,307.5 \t"}),r.a.createElement("polygon",{className:"st8",points:"224.5,307.5 227.1,309.1 206.8,321.2 186.8,321.2 186.8,309.3 \t"}),r.a.createElement("polygon",{className:"st9",points:"303,290.1 303,359.9 274.9,376.2 250.5,390.2 186.8,427 186.8,321.2 206.8,321.2 226.7,365.7  212.6,390.2 228.8,390.2 260.2,355.7 251.8,342.8 246.4,334.5 230,309.1 228.8,307.3 186.8,309.3 186.8,290.5 232.9,290.5  240.7,304.1 253.2,325.7 258.9,335.6 \t"})),D=r.a.createElement("g",{id:"Layer_2"},r.a.createElement("path",{className:"st10",d:"M296.8,187.7l-2.8-7.1c-1.4-3.2-4.5-5.2-7.9-5.2h-71c-3.4,0-6.5,2-7.9,5.2l-2.8,7.1h-28.4l-2.9-7.2 c-1.3-3.1-4.4-5.1-7.8-5.1H94.1c-3.4,0-6.4,2-7.8,5.1l-2.9,7.2h-13v8.7h10.3c0,0.5,0.1,1,0.2,1.4l6.2,29.4c0.8,3.9,4.3,6.7,8.3,6.7 H164c4,0,7.4-2.8,8.3-6.7l6.2-29.4c0.1-0.5,0.2-1,0.2-1.4h23c0,0.5,0.1,1,0.2,1.5l6.1,29.2c0.8,4,4.4,6.9,8.4,6.9h68.3 c4.1,0,7.6-2.9,8.4-6.9l6.1-29.2c0.1-0.5,0.2-1,0.2-1.5h3.7v-8.7H296.8z M173.7,199.4l-5.2,24.7c-0.7,3.5-3.8,6.1-7.4,6l-62.6-0.7 c-3.6,0-6.7-2.6-7.5-6.2l-4.9-24.8c-0.1-0.6-0.2-1.3-0.2-1.9c0-0.9,0.2-1.9,0.6-2.7l1.9-6l1.3-4.2c1.2-2.8,3.9-4.6,6.9-4.5l65,0.8 c3,0,5.8,1.9,7,4.7l1.4,3.2l3.1,7c0.2,0.5,0.4,1.1,0.5,1.7C173.9,197.4,173.9,198.4,173.7,199.4z M294.8,200l-5.2,24.7 c-0.7,3.5-3.8,6.1-7.4,6l-62.6-0.7c-3.6,0-6.7-2.6-7.5-6.2l-5.9-24.8c-0.2-0.9-0.2-1.7-0.1-2.6c0.1-0.7,0.3-1.4,0.5-2l2.8-6.6 l1.5-3.5c1.2-2.8,3.9-4.6,6.9-4.5l64.9,0.8c3,0,5.8,1.9,7,4.7l1.1,2.6l3.4,7.7c0.2,0.3,0.3,0.7,0.4,1 C295,197.6,295.1,198.8,294.8,200z"}),r.a.createElement("path",{className:"st11",d:"M294.8,200l-5.2,24.7c-0.7,3.5-3.8,6.1-7.4,6l-62.6-0.7c-3.6,0-6.7-2.6-7.5-6.2l-5.9-24.8 c-0.2-0.9-0.2-1.7-0.1-2.6c0.1-0.7,0.3-1.4,0.5-2l2.8-6.6l1.5-3.5c1.2-2.8,3.9-4.6,6.9-4.5l64.9,0.8c3,0,5.8,1.9,7,4.7l1.1,2.6 l3.4,7.7c0.2,0.3,0.3,0.7,0.4,1C295,197.6,295.1,198.8,294.8,200z"}),r.a.createElement("g",{className:"st12"},r.a.createElement("path",{className:"st13",d:"M173.7,199.4l-5.2,24.7c-0.7,3.5-3.8,6.1-7.4,6l-62.6-0.7c-3.6,0-6.7-2.6-7.5-6.2l-4.9-24.8 c-0.1-0.6-0.2-1.3-0.2-1.9c0-0.9,0.2-1.9,0.6-2.7l1.9-6l1.3-4.2c1.2-2.8,3.9-4.6,6.9-4.5l65,0.8c3,0,5.8,1.9,7,4.7l1.4,3.2l3.1,7 c0.2,0.5,0.4,1.1,0.5,1.7C173.9,197.4,173.9,198.4,173.7,199.4z"})),r.a.createElement("path",{className:"st14",d:"M86,93.4c14.9-5.3,39.7-20.8,51.8-20.9s23.8-16.8,23.8-16.8h-57.7L86,93.4z"}),r.a.createElement("path",{className:"st3",d:"M186.8,72.6c0,0-20.3-0.4-49,0S89.9,84.4,86,93.4c0,0-14.4-47.5,11.1-69.5s68-30.7,89.7-18.2"})),q=r.a.createElement("polygon",{className:"st15",points:"186.8,290.5 175.6,278.3 151.8,284.4 141.2,274 156.2,278.3 169.5,269.3 186.6,274.8 "}),F=r.a.createElement("polygon",{className:"st16",points:"186.8,290.5 198.3,278.3 222.7,284.4 233.5,274 218.2,278.3 204.6,269.3 187,274.8 "}),G=r.a.createElement("polygon",{className:"st3",points:"88.8,173.6 129.4,162.1 173.6,163.8 165.7,172.9 129.9,168.8 "}),U=r.a.createElement("polygon",{className:"st9",points:"290.9,173.6 250.3,162.1 206.1,163.8 214,172.9 249.8,168.8 "}),_=function(e){return r.a.createElement("svg",R({version:1.1,id:"Layer_1",xmlnsXlink:"http://www.w3.org/1999/xlink",x:"0px",y:"0px",viewBox:"0 0 373.6 452.1",style:{enableBackground:"new 0 0 373.6 452.1"},xmlSpace:"preserve"},e),W,D,q,F,G,U)};n.p;function Y(){var e=Object(a.useState)(!1),t=Object(P.a)(e,2),n=t[0],o=t[1];function i(){n||(o(!0),setTimeout(function(){return o(!1)},3e3))}return r.a.createElement("div",{className:H()("avatar",{active:n})},r.a.createElement("div",{className:"note-wrapper n1"},r.a.createElement("div",{className:"note"},"\u266b")),r.a.createElement("div",{className:"note-wrapper n2"},r.a.createElement("div",{className:"note"},"\u266a")),r.a.createElement("div",{className:"note-wrapper n3"},r.a.createElement("div",{className:"note"},"\u266b")),r.a.createElement("div",{className:"note-wrapper n4"},r.a.createElement("div",{className:"note"},"\u266a")),r.a.createElement(_,{onClick:i,onMouseOver:i}))}function J(e){var t=e.showBack;return r.a.createElement("div",{className:"header"},r.a.createElement(Y,null),r.a.createElement(v.a,{to:"/",className:"title"},r.a.createElement("span",{className:"a"},"dijs"),r.a.createElement("span",{className:"b"},"talks")),r.a.createElement("div",{className:"bottom"},t?r.a.createElement(B,null):null,r.a.createElement("div",{className:"social"},r.a.createElement("a",{href:"https://github.com/dijs",rel:"noopener noreferrer",target:"_blank"},r.a.createElement(E,{className:"icon"})),r.a.createElement("a",{href:"https://www.linkedin.com/in/richard-van-der-dys-iii-91880a24/",rel:"noopener noreferrer",target:"_blank"},r.a.createElement(A,{className:"icon"})),r.a.createElement("a",{href:"https://npmjs.com/~dijs",rel:"noopener noreferrer",target:"_blank"},r.a.createElement(O,{className:"icon"})))))}function V(e){var t=e.metadata,n=t.title,a=t.date,o=t.blurb,i=e.content,s=e.slug;return r.a.createElement("div",null,r.a.createElement(b.a,null,r.a.createElement("title",null,n),r.a.createElement("meta",{property:"og:url",content:"https://blog.richardvanderdys.com/post/".concat(s)}),r.a.createElement("meta",{property:"og:type",content:"article"}),r.a.createElement("meta",{property:"og:title",content:n}),r.a.createElement("meta",{property:"og:description",content:o})),r.a.createElement(J,{showBack:!0}),r.a.createElement("article",{className:"post"},r.a.createElement("div",{className:"info"},r.a.createElement("h1",null,n),r.a.createElement("div",{className:"date"},a)),r.a.createElement("div",{className:"content"},r.a.createElement(w.a,{source:i,escapeHtml:!1}))),r.a.createElement(B,null))}function $(e){var t=e.metadata,n=t.title,a=t.blurb,o=t.date,i=(e.slug,e.path);return r.a.createElement(v.a,{className:"post-item",to:i},r.a.createElement("div",{className:"title"},n),r.a.createElement("div",{className:"blurb"},a),r.a.createElement("div",{className:"date"},o))}function K(e){var t=e.posts;return r.a.createElement("div",{className:"posts"},t.map(function(e){return r.a.createElement($,Object.assign({key:e.slug},e))}))}function X(){return r.a.createElement("div",null,r.a.createElement(b.a,null,r.a.createElement("title",null,"Richard van der Dys | Blog"),r.a.createElement("meta",{property:"og:url",content:"https://blog.richardvanderdys.com"}),r.a.createElement("meta",{property:"og:type",content:"article"}),r.a.createElement("meta",{property:"og:title",content:"Richard van der Dys | Blog"}),r.a.createElement("meta",{property:"og:description",content:"Collection of posts about my personal development"})),r.a.createElement(J,null),r.a.createElement(K,{posts:g}))}var Q=n(103),Z=n(104),ee=n(109),te=n(105),ne=n(110),ae=n(343),re=function(e){function t(){return Object(Q.a)(this,t),Object(ee.a)(this,Object(te.a)(t).apply(this,arguments))}return Object(ne.a)(t,e),Object(Z.a)(t,[{key:"componentDidUpdate",value:function(e){this.props.location.pathname!==e.location.pathname&&window.scrollTo(0,0)}},{key:"render",value:function(){return this.props.children}}]),t}(r.a.Component),oe=Object(ae.a)(re);var ie=function(){var e=g.map(function(e){return{title:e.metadata.title,path:e.path,component:function(){return r.a.createElement(V,e)},exact:!0}});return r.a.createElement(i.a,null,r.a.createElement(oe,null,r.a.createElement(s.a,null,e.map(function(e,t){return r.a.createElement(l.a,Object.assign({key:t},e))}),r.a.createElement(l.a,{component:X}))))},se=Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));function le(e,t){navigator.serviceWorker.register(e).then(function(e){e.onupdatefound=function(){var n=e.installing;null!=n&&(n.onstatechange=function(){"installed"===n.state&&(navigator.serviceWorker.controller?(console.log("New content is available and will be used when all tabs for this page are closed. See http://bit.ly/CRA-PWA."),t&&t.onUpdate&&t.onUpdate(e)):(console.log("Content is cached for offline use."),t&&t.onSuccess&&t.onSuccess(e)))})}}).catch(function(e){console.error("Error during service worker registration:",e)})}n(337);Object(o.render)(r.a.createElement(ie,null),document.getElementById("root")),function(e){if("serviceWorker"in navigator){if(new URL("",window.location.href).origin!==window.location.origin)return;window.addEventListener("load",function(){var t="".concat("","/service-worker.js");se?(function(e,t){fetch(e).then(function(n){var a=n.headers.get("content-type");404===n.status||null!=a&&-1===a.indexOf("javascript")?navigator.serviceWorker.ready.then(function(e){e.unregister().then(function(){window.location.reload()})}):le(e,t)}).catch(function(){console.log("No internet connection found. App is running in offline mode.")})}(t,e),navigator.serviceWorker.ready.then(function(){console.log("This web app is being served cache-first by a service worker. To learn more, visit http://bit.ly/CRA-PWA")})):le(t,e)})}}()}},[[111,2,1]]]);
//# sourceMappingURL=main.792a49d8.chunk.js.map